{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAMLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a surface code noisy line and sample it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import stim\n",
    "circuit = stim.Circuit.generated(\"surface_code:rotated_memory_x\", \n",
    "                                 distance=3, \n",
    "                                 rounds=1, \n",
    "                                 after_clifford_depolarization=0.05)\n",
    "num_shots = 1000\n",
    "model = circuit.detector_error_model(decompose_errors=False, flatten_loops=True)\n",
    "sampler = circuit.compile_detector_sampler()\n",
    "syndrome, actual_observables = sampler.sample(shots=num_shots, separate_observables=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an MLD decoder and perform decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/1000\n"
     ]
    }
   ],
   "source": [
    "import hamld\n",
    "mld_decoder = hamld.HAMLD(detector_error_model=model, order_method='mld', slice_method='no_slice')\n",
    "predicted_observables = mld_decoder.decode_batch(syndrome)\n",
    "num_mistakes = np.sum(np.any(predicted_observables != actual_observables, axis=1))\n",
    "\n",
    "print(f\"{num_mistakes}/{num_shots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an HMLD decoder and perform decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/1000\n"
     ]
    }
   ],
   "source": [
    "import hamld\n",
    "hmld_decoder = hamld.HAMLD(detector_error_model=model, order_method='greedy', slice_method='no_slice')\n",
    "predicted_observables = hmld_decoder.decode_batch(syndrome)\n",
    "num_mistakes = np.sum(np.any(predicted_observables != actual_observables, axis=1))\n",
    "\n",
    "print(f\"{num_mistakes}/{num_shots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an HAMLD decoder and perform decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/1000\n"
     ]
    }
   ],
   "source": [
    "import hamld\n",
    "\n",
    "hamld_decoder =  hamld.HAMLD(detector_error_model=model,\n",
    "                            order_method='greedy',\n",
    "                            slice_method='no_slice',\n",
    "                            use_approx = True,\n",
    "                            approximatestrategy = \"hyperedge_topk\",\n",
    "                            approximate_param = 100,\n",
    "                            contraction_code = \"normal\",\n",
    "                            accuracy = \"float64\",\n",
    "                            priority = -2,\n",
    "                            priority_topk= 150)\n",
    "\n",
    "predicted_observables = hamld_decoder.decode_batch(syndrome)\n",
    "num_mistakes = np.sum(np.any(predicted_observables != actual_observables, axis=1))\n",
    "                            \n",
    "print(f\"{num_mistakes}/{num_shots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing: group together the same (syndrome, logical observable) and decode once, implementing a decoding method similar to a look-up table.\n",
    "\n",
    "After data preprocessing, decoding is performed.\n",
    "\n",
    "It does not affect the decoding accuracy, but can reduce the time required for QEC experiments.\n",
    "\n",
    "For more information, please refer to the code in the benchmarking directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/1000\n"
     ]
    }
   ],
   "source": [
    "sample = np.hstack((syndrome, actual_observables))\n",
    "len_syndrome = syndrome.shape[1]\n",
    "unique_sample, counts = np.unique(sample, axis = 0, return_counts=True)\n",
    "num_unique_sample = unique_sample.shape[0]\n",
    "unique_syndrome = unique_sample[:,:len_syndrome]\n",
    "unique_actual_observables = unique_sample[:,len_syndrome:]\n",
    "\n",
    "hmld_decoder = hamld.HAMLD(detector_error_model=model, order_method='greedy', slice_method='no_slice')\n",
    "\n",
    "unique_predicted_observables = hmld_decoder.decode_batch(unique_syndrome)\n",
    "mistakes_mask = np.any(unique_predicted_observables != unique_actual_observables, axis=1)\n",
    "num_mistakes = np.sum(mistakes_mask * counts)\n",
    "\n",
    "print(f\"{num_mistakes}/{num_shots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode a syndrome, where after each decoding, the output is a probability distribution. We need to select the logical operation corresponding to the syndrome with a large probability for error correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False, False, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syndrome = syndrome[0]\n",
    "syndrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]),\n",
       " {(False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   True): 0.0010887631938922256,\n",
       "  (False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False): 0.019538367135331285},\n",
       " 0.947216933402039)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mld_decoder.decode(syndrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]),\n",
       " {(False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   True): 0.0010887631938922256,\n",
       "  (False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False): 0.019538367135331285},\n",
       " 0.947216933402039)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmld_decoder.decode(syndrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]),\n",
       " {(False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   True): 0.0010887631938922256,\n",
       "  (False,\n",
       "   False,\n",
       "   False,\n",
       "   True,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False,\n",
       "   False): 0.019538367135331285},\n",
       " 0.947216933402039)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamld_decoder.decode(syndrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoding tasks between different syndromes are entirely independent, so in specific experimental tests, we can use Python's multiprocessing to perform parallel decoding.  \n",
    "\n",
    "The speedup ratio depends on the number of CPU cores. The `parallel_decode_batch` function can also take a `num_workers` parameter; otherwise, it will use half of `multiprocessing.cpu_count()`.  \n",
    "\n",
    "Through parallel decoding, the time required for error correction experiments can be significantly reduced in large-scale quantum error correction simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628/10000\n"
     ]
    }
   ],
   "source": [
    "circuit = stim.Circuit.generated(\"surface_code:rotated_memory_x\", \n",
    "                                distance=3, \n",
    "                                rounds=1, \n",
    "                                after_clifford_depolarization=0.05)\n",
    "num_shots = 10000\n",
    "model = circuit.detector_error_model(decompose_errors=False, flatten_loops=True)\n",
    "sampler = circuit.compile_detector_sampler()\n",
    "syndrome, actual_observables = sampler.sample(shots=num_shots, separate_observables=True)\n",
    "\n",
    "import hamld\n",
    "hmld_decoder = hamld.HAMLD(detector_error_model=model, order_method='greedy', slice_method='no_slice')\n",
    "predicted_observables = hmld_decoder.parallel_decode_batch(syndrome)\n",
    "\n",
    "num_mistakes = np.sum(np.any(predicted_observables != actual_observables, axis=1))\n",
    "\n",
    "print(f\"{num_mistakes}/{num_shots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain scenarios, we can enable detailed analysis by outputting the probability distribution. By setting output_prob=True, the system will not only provide the predicted logical error for syndrome correction, but also return the corresponding probability distribution for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10\n",
      "predicted_observables: (10, 1)\n",
      "prob_dists: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "circuit = stim.Circuit.generated(\"surface_code:rotated_memory_x\", \n",
    "                                distance=3, \n",
    "                                rounds=1, \n",
    "                                after_clifford_depolarization=0.05)\n",
    "num_shots = 10\n",
    "model = circuit.detector_error_model(decompose_errors=False, flatten_loops=True)\n",
    "sampler = circuit.compile_detector_sampler()\n",
    "syndrome, actual_observables = sampler.sample(shots=num_shots, separate_observables=True)\n",
    "\n",
    "import hamld\n",
    "hmld_decoder = hamld.HAMLD(detector_error_model=model, order_method='greedy', slice_method='no_slice')\n",
    "predicted_observables, prob_dists = hmld_decoder.decode_batch(syndrome, output_prob=True)\n",
    "\n",
    "num_mistakes = np.sum(np.any(predicted_observables != actual_observables, axis=1))\n",
    "\n",
    "print(f\"{num_mistakes}/{num_shots}\")\n",
    "print(f\"predicted_observables: {predicted_observables.shape}\")\n",
    "print(f\"prob_dists: {prob_dists.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output_prob parameter remains configurable in parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/100\n",
      "predicted_observables: (100, 1)\n",
      "prob_dists: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "circuit = stim.Circuit.generated(\"surface_code:rotated_memory_x\", \n",
    "                                distance=3, \n",
    "                                rounds=1, \n",
    "                                after_clifford_depolarization=0.05)\n",
    "num_shots = 100\n",
    "model = circuit.detector_error_model(decompose_errors=False, flatten_loops=True)\n",
    "sampler = circuit.compile_detector_sampler()\n",
    "syndrome, actual_observables = sampler.sample(shots=num_shots, separate_observables=True)\n",
    "\n",
    "import hamld\n",
    "hamld_decoder = hamld.HAMLD(detector_error_model=model, order_method='greedy', slice_method='no_slice')\n",
    "predicted_observables, prob_dists = hamld_decoder.parallel_decode_batch(syndrome, output_prob=True)\n",
    "\n",
    "num_mistakes = np.sum(np.any(predicted_observables != actual_observables, axis=1))\n",
    "\n",
    "print(f\"{num_mistakes}/{num_shots}\")\n",
    "print(f\"predicted_observables: {predicted_observables.shape}\")\n",
    "print(f\"prob_dists: {prob_dists.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用高性能实现的C++接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考README.md编译c++接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hamld import HAMLDCpp_from_file\n",
    "# from hamld.benchmark import generate_detector_error_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_syndrome: [0 0 1 0]\n",
      "predicted_observable: [False]\n",
      "prob_dist: {'0010': 0.009524717155758559}\n"
     ]
    }
   ],
   "source": [
    "# d = 3\n",
    "# r = 1\n",
    "# # real p is 10/10000\n",
    "# p = 10\n",
    "# noise_model = \"si1000\"\n",
    "# error_type = \"Z\"\n",
    "# related_path = \"../data/external/epmld_experiment_data/epmld_paper_experiment/overall_performance/surface_code\"\n",
    "# have_stabilizer = False\n",
    "\n",
    "# approximatestrategy = \"hyperedge_topk\"\n",
    "# approximate_param = 3\n",
    "# priority = 0\n",
    "# priority_topk = 3\n",
    "# dem_file = generate_detector_error_model_path(d = d, r=r, p=p, noise_model=noise_model,\n",
    "#                                               error_type=error_type, decomposed_error = False,\n",
    "#                                               related_path=related_path, have_stabilizer = have_stabilizer)\n",
    "# decoder =  HAMLDCpp_from_file(dem_path=str(dem_file), \n",
    "#                               approx_strategy=str(approximatestrategy), \n",
    "#                               approx_param=float(approximate_param), \n",
    "#                               priority=int(priority), \n",
    "#                               priority_topk = int(priority_topk),\n",
    "#                               use_heuristic = False, alpha = float(0.05))\n",
    "\n",
    "# # if have_stabilizer = False, surface code.\n",
    "# detector_number = int((d**2-1)*r - (d**2-1)/2)\n",
    "# random_syndrome = np.random.randint(0, 2, detector_number)\n",
    "# print(\"random_syndrome:\", random_syndrome)\n",
    "# predicted_observable, prob_dist, relative_correct_probability = decoder.decode(random_syndrome)\n",
    "# print(\"predicted_observable:\", predicted_observable)\n",
    "# # The syndrome is remapped to a new one, so sometimes random_syndrome and the syndrome in prob_dist differ.\n",
    "# print(\"prob_dist:\", prob_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ck_hamld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
